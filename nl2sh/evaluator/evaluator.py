# we use gpt-5.1 as the judge: cheap while powerful
from __future__ import annotations

from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

from nl2sh.agents.llm_service import LLMService
from nl2sh.prompts.eval_pmpt import eval_prompt
from typing import Dict, Any, List, Tuple
import json
from tqdm.auto import tqdm

class Evaluator:
    """
    Evaluator using LLM to judge the quality of generated bash commands.
    It uses a prompt template to ask the LLM to score the command based on how well it fulfills the task description.
    The score is expected to be a value in [0, 10].
    Attributes:
        model (str): The LLM model to use for evaluation.
        template (str): The prompt template for evaluation.
        instance (LLMService): An instance of the LLM service for making requests.
    Methods:
        eval_batch: Evaluate a batch of (task, command) pairs using multiple workers.
        eval_from_file: Evaluate (task, command) pairs read from an input file generated by `Inference` class and write results to an output file.
        _eval_one: Evaluate a single (task, command) pair and return the score.
    """

    def __init__(self, model: str = 'gpt-5.1'):
        self.model = model
        self.template = eval_prompt
        self.instance = LLMService(model)

    def _eval_one(self, task: str, command: str) -> float:
        """
        Evaluate a single (task, command) pair and return the score.
        Args:
            task (str): The natural language task description.
            command (str): The generated bash command.
        Returns:
            float: The score assigned by the LLM. It is expected to be a integer in [0, 10], but to be safe we use float.
        """

        # in this prompt, we ask the LLM to output only the score number
        prompt = ((self.template
                  .replace("{{TASK_DESCRIPTION}}", task))
                  .replace("{{BASH_COMMAND}}", command))
        prompt_set = [
            {"role": "user", "content": prompt},
        ]
        res = self.instance.chat(prompt_set)
        if not res:
            raise ValueError("The LLM said nothing")
        return float(res.strip())

    def eval_batch(
            self,
            pairs: List[Tuple[str, str, int]],
            num_workers: int = 5,
            ofile: str | Path | None = None,
    ) -> list[tuple[str, str, int]] | tuple[list[tuple[str, str, int]], float]:
        """
        Evaluate a batch of (task, command) pairs using multiple workers.
        Args:
            pairs (List[Tuple[str, str, int]]): A list of (task, command, dummy_score) tuples to evaluate.
            num_workers (int): The number of parallel workers to use for evaluation.
            ofile (str | Path | None): Optional output file path to save the results.
        Returns:
            List[Tuple[str, str, int]]: A list of (task, command, score) tuples with the evaluated scores.
        """
        results: List[Tuple[str, str, int]] = []    # (task, cmd, score)
        total_score = 0     # total score accumulator
        if not pairs:
            return results

        # use ThreadPoolExecutor for parallel evaluation to accelerate the process. LLM calls are mostly API IO-bound, so it's not limited by GIL.
        with ThreadPoolExecutor(max_workers=num_workers) as ex:
            future_to_pair = {
                ex.submit(self._eval_one, task, cmd): (task, cmd)
                for (task, cmd, _) in pairs
            }

            for fut in tqdm(
                    as_completed(future_to_pair),
                    total=len(future_to_pair),
                    desc=f"Judging commands (workers={num_workers})",
                    unit="case",
            ):
                task, cmd = future_to_pair[fut]
                try:
                    score = fut.result()
                except Exception as e:
                    print(f"[WARN] judging failed for task: {task!r}, cmd: {cmd!r}, err: {e}")
                    score = -1
                results.append((task, cmd, score))
                total_score+=score

        # if ofile is given, save the results to the file.
        if ofile is not None:
            ofile = Path(ofile)
            with ofile.open("w", encoding="utf-8") as f:
                for task, cmd, score in results:
                    rec: Dict[str, Any] = {
                        "task": task,
                        "command": cmd,
                        "score": score,
                    }
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                # f.write(f"Total score: {total_score}, avg_score = {total_score / len(results)}\n")
            print(f"total score: {total_score}, avg_score = {total_score / len(results)}")
            print(f"Saved {len(results)} judged records to {ofile}")

        # no matter of ofile, we return the results and average score
        return results, total_score/len(results)

    def eval_from_file(
            self,
            infile: str | Path,
            outfile: str | Path,
            num_workers: int = 5,
    ) -> list[tuple[str, str, int]] | tuple[list[tuple[str, str, int]], float]:
        """
            Evaluate (task, command) pairs read from an input file generated by `Inference` class and write results to an output file.
            Args:
                infile (str | Path): The input file path containing (task, command) pairs in JSON lines format.
                outfile (str | Path): The output file path to save the evaluation results.
                num_workers (int): The number of parallel workers to use for evaluation.
            Returns:
                List[Tuple[str, str, int]]: A list of (task, command, score) tuples with the evaluated scores.
        """

        infile = Path(infile)
        pairs: List[Tuple[str, str, int]] = []

        with infile.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                obj = json.loads(line)
                task = obj.get("task", "")
                cmd = obj.get("command", "")
                if task and cmd:
                    pairs.append((task, cmd, 0))

        return self.eval_batch(pairs, num_workers=num_workers, ofile=outfile)


if __name__ == "__main__":
    q = "print the number of processors"
    a = "task: cat /proc/cpuinfo | fgrep \"CPU(s)\" | head -n 1 | awk '{print $4}'| tac"
    ev = Evaluator()
    ev.eval_from_file(
        "gened_files/FT_ed_o.txt",
        "eval_results/FT_o_res.txt",
    )
