{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Final Project: Natural Language to Shell Command (NL2Sh)**\n",
    "- This system will translate the user's natural language description of the task to shell command. With three agents: clarifier, composer, and inspector, the system will explain the user's task, compose a command, and inspect the command. If there is a mistake, a guide will be proposed by the inspector.\n",
    "- This is the demo notebook of our project, which include the most steps from data preparation, agent pipeline initialization, inference, and E2E evaluation.\n",
    ">P.S: Fine-tune steps will be finished on OpenAI Platform, so the code of this part is not included."
   ],
   "id": "1d2418f1ad5277db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_timestamp = datetime.now()\n",
    "print(f\"Start time: {start_timestamp.strftime('%H:%M:%S')}\")"
   ],
   "id": "55d01d25fbf06bf7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install Environment",
   "id": "9e025a93905ac481"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create virtual environment and install all packages needed\n",
    "!python -m venv .venv\n",
    "!source .venv/bin.activate\n",
    "!pip install ."
   ],
   "id": "19f41ed31f89a511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Remember to select the kernel from .venv !!!**",
   "id": "78fdf2d85bf71eef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare datasets\n",
    "- We use the `westenfelder/NL2SH-ALFA` dataset with 40k train samples and a test sets with 300 data.\n",
    "- This dataset contains natural languages description of tasks and the corresponding shell commands.\n",
    "- Because of the limitation in time and computes, we only use 1000 samples in the train set and 50 samples as our validation set.\n",
    "- More information can be seen [here](https://huggingface.co/datasets/westenfelder/NL2SH-ALFA)."
   ],
   "id": "63b1e25dde3bb865"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import necessary functions\n",
    "from nl2sh.data.dataloader import generate_eval_data, generate_finetune_data, generate_validation_data"
   ],
   "id": "52864314d7ac2db2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create a 1k-data train set",
   "id": "215e88a21669eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "generate_finetune_data('data/train_set.jsonl') # make sure it's a jsonl",
   "id": "44aafb743d22c9f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Then you will see a `train_set.jsonl` in the `data` dir.",
   "id": "e3e32fea494554f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create Validation set",
   "id": "a51ff63f2015e9fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "generate_validation_data(\"data/val_set.jsonl\")",
   "id": "ad50e923ae8338fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we have two jsonl's in the `data` folder. We used these two dataset in the fine-tuning of our model. Configuration:\n",
    "```text\n",
    "Model = GPT-4o-mini,\n",
    "Epoch = 1,\n",
    "Batch size = 1,\n",
    "LR multiplier = 1.8,\n",
    "Seed = 114514\n",
    "```"
   ],
   "id": "4648bca2148409c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Next, we create our evaluation set with 50 randomly selected data from the train set.\n",
    "- Here the difficulties are evenly distributed."
   ],
   "id": "cfad06d148b49595"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "generate_eval_data(\"data/eval_set.jsonl\")",
   "id": "146bd0b314ae7e29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Next, let's try one example with this example",
   "id": "de2ab5d847545a7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from nl2sh.inference import Inference, load_evaluation_nl",
   "id": "1733de605fbc951d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create an instance with the default configurations",
   "id": "d31ade410a6c9ad2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "system_default = Inference()",
   "id": "943dc7d832394936",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# because this is a simple task and to save time, we set the max_recompose to 1.\n",
    "task = 'Show all files and folders in the current directory'\n",
    "resp = system_default.run_single(task = task, max_recompose= 1)\n",
    "print(f\"To handle task {task}, you need to run {resp}.\")"
   ],
   "id": "408e8d631361e3f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Seems interesting... Now let's start our evaluation! Let's see how our default configuration performs!",
   "id": "fc6ae763a60dc04b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load the test_set",
   "id": "991a2fc9fed44fa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sets = load_evaluation_nl(\n",
    "    path = 'data/eval_set.jsonl',\n",
    ")"
   ],
   "id": "966ecd3479bb2695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Let's generate the answers for the test_set",
   "id": "78d3c4103777140a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ans_default = system_default.gen_eval_commands(test_sets, max_recompose= 2, ofile = 'eval_results/default_ans.txt') # let the max_recompose be 2 to make sure it has 3 times to correct the mistakes",
   "id": "c856102d567aeb7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Next, let a judge to grade the answers.",
   "id": "52f10767c160d974"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nl2sh.evaluator.evaluator import Evaluator\n",
    "evaluator = Evaluator()"
   ],
   "id": "db15c8b25a492e22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lst_dft, avg_dft = evaluator.eval_batch(ans_default, 5, './eval_results/default.txt')",
   "id": "bfac479541255f1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### [Experiment 1] How will our fine-tuned model perform on the same dataset?",
   "id": "346564ed83f3d1c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "system_ft = Inference(use_finetune=True)",
   "id": "d10c794d1f68dff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Notice we are using our own model!",
   "id": "5ea43ad2cf33a5e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate answers.",
   "id": "7e8e5ac3a5829500"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ans_ft = system_ft.gen_eval_commands(test_sets, max_recompose= 2, ofile = 'eval_results/finetune_ans.txt')",
   "id": "6a5eda5e771e9a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lst_ft, avg_ft = evaluator.eval_batch(ans_ft, 5, './eval_results/finetune.txt')",
   "id": "b521b11d67882eda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### [Experiment 2][Ablation] What if we make our inspector a less powerful model?",
   "id": "44e23b2b8dd16ae4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "system_abl = Inference(inspect_abltn=True)",
   "id": "dd00bdb53dff5752",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ">Notice that Inspector is now gpt-4o-mini",
   "id": "d93d10c7db852c49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generate Answers",
   "id": "1e20e54f46bb167c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ans_abl = system_abl.gen_eval_commands(test_sets, max_recompose= 2, ofile = 'eval_results/abl_ans.txt')",
   "id": "fbc33126e34f651a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Grade the answers",
   "id": "69f6c800a9276acc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "lst_abl, avg_abl = evaluator.eval_batch(ans_abl, 5, './eval_results/ablation.txt')",
   "id": "4c5a97ba3bddb34b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stats and Visualization",
   "id": "643449a4a8678ffd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ],
   "id": "e9272330b291fa79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's check the scores of the three cases!",
   "id": "fc0299ca84bab78e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scores_default = [item[2] for item in lst_dft]\n",
    "scores_ft = [item[2] for item in lst_ft]\n",
    "scores_abl = [item[2] for item in lst_abl]\n",
    "# we have already got avg_dft, avg_ft, avg_abl"
   ],
   "id": "eb3be9a34ff7d267",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_pack = [\n",
    "    (\"Default Model\", scores_default, avg_dft),\n",
    "    (\"Fine-Tuned\",    scores_ft,      avg_ft),\n",
    "    (\"Ablation\",      scores_abl,     avg_abl),\n",
    "]\n",
    "\n",
    "cmap = plt.get_cmap('RdYlGn')\n",
    "\n",
    "def get_color(score):\n",
    "    return cmap(score / 10.0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for ax, (title, data_list, avg_val) in zip(axes, data_pack):\n",
    "    counts = Counter(data_list)\n",
    "\n",
    "    sorted_scores = sorted(counts.keys())\n",
    "    sizes = [counts[s] for s in sorted_scores]\n",
    "\n",
    "    chart_colors = [get_color(s) for s in sorted_scores]\n",
    "    labels = [f\"{s} points\" for s in sorted_scores]\n",
    "\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        sizes,\n",
    "        labels=labels,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=chart_colors,\n",
    "        pctdistance=0.85,\n",
    "        wedgeprops=dict(width=0.4, edgecolor='w', linewidth=1)\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=14, pad=15)\n",
    "\n",
    "    ax.text(0, 0,\n",
    "            f\"Avg Score\\n{avg_val:.1f}\",\n",
    "            ha='center', va='center',\n",
    "            fontsize=14, fontweight='bold', color='#333333')\n",
    "\n",
    "plt.suptitle(\"Model Evaluation Scores (0-10)\", fontsize=16, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "plt.show()"
   ],
   "id": "6bea46f564e25ef3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's check the retry time of the three cases!",
   "id": "13eefb7c21362604"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_retry_default = [item[2] for item in ans_default]\n",
    "avg_retry_default = (0.0+sum(n_retry_default)/len(n_retry_default))\n",
    "\n",
    "n_retry_ft = [item[2] for item in ans_ft]\n",
    "avg_retry_ft = (0.0+sum(n_retry_ft)/len(n_retry_ft))\n",
    "\n",
    "n_retry_abl = [item[2] for item in ans_abl]\n",
    "avg_retry_abl = (0.0+sum(n_retry_abl)/len(n_retry_abl))"
   ],
   "id": "98e4e000599ddbac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_pack = [\n",
    "    (\"Default Model\", n_retry_default, avg_retry_default),\n",
    "    (\"Fine-Tuned\",    n_retry_ft,      avg_retry_ft),\n",
    "    (\"Ablation\",      n_retry_abl,     avg_retry_abl),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "colors_map = {\n",
    "    0: '#66b3ff',\n",
    "    1: '#ffcc99',\n",
    "    2: '#ff9999'\n",
    "}\n",
    "labels_map = [\"0 Retries\", \"1 Retry\", \"2 Retries\"]\n",
    "\n",
    "for ax, (title, data_list, avg_val) in zip(axes, data_pack):\n",
    "    counts = Counter(data_list)\n",
    "    sizes = [counts.get(k, 0) for k in [0, 1, 2]]\n",
    "\n",
    "    chart_colors = [colors_map[k] for k in [0, 1, 2]]\n",
    "\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        sizes,\n",
    "        labels=labels_map,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=chart_colors,\n",
    "        pctdistance=0.85,\n",
    "        wedgeprops=dict(width=0.4, edgecolor='w')\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "\n",
    "    ax.text(0, 0,\n",
    "            f\"Avg Retry\\n{avg_val:.2f}\",\n",
    "            ha='center', va='center',\n",
    "            fontsize=15, fontweight='bold', color='#333333')\n",
    "\n",
    "plt.suptitle(\"Retry Distribution & Average Performance\", fontsize=22, y= 0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ],
   "id": "9a56fdb23af3c7a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### To be more clear, let's compare the averages directly!",
   "id": "615dce9ac36fed24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "models = ['Default', 'Fine-Tuned', 'Ablation']\n",
    "avg_scores = [avg_dft, avg_ft, avg_abl]\n",
    "avg_retries = [avg_retry_default, avg_retry_ft, avg_retry_abl]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, avg_scores, width, label='Avg Score (0-10)', color='#99ff99', alpha=0.9)\n",
    "ax1.set_ylabel('Average Score', color='green', fontweight='bold')\n",
    "ax1.set_ylim(0, 11)\n",
    "ax1.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "bars2 = ax2.bar(x + width/2, avg_retries, width, label='Avg Retries', color='#ff9999', alpha=0.9)\n",
    "ax2.set_ylabel('Avg Retry Count', color='red', fontweight='bold')\n",
    "ax2.set_ylim(0, max(avg_retries) * 1.5)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Final Showdown: Score vs. Efficiency', fontsize=16, pad=20)\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "def add_labels(bars, ax):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "add_labels(bars1, ax1)\n",
    "add_labels(bars2, ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "31893677011f6492",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observations\n",
    "### Default vs. Ablation\n",
    "- From the two plots shown above, it can be seen that the default setting has the similar score to that of the ablation setting. In multiple experiments, sometimes the former scored higher than the latter, and sometimes the latter scored higher than the former.\n",
    "- However, from multiple experiments, it is always observed that the default setting has a little higher number of retries than does the ablation settings.\n",
    "- As the default setting uses `GPT-5.1` model as the inspector, and the ablation setting uses a much less powerful `GPT-4o-mini` model, the former inspector is able to produce a much more rigorous judge for the commands composed by the composer and find out the tiny mistakes that may be ignored by `GPT-4o-mini`, so the default setting has a little higher retry-times.\n",
    "\n",
    "### Default vs. Fine-tuned\n",
    "- Fine-tuned system works much worse than does the default settings in both metrics: it has obviously lower score and higher number of retry.\n",
    "- This degradation is not entirely surprising. We only fine-tuned on a relatively small dataset of 1k examples and for a single epoch, which provides limited coverage of the full NL2Bash distribution and makes the model prone to overfitting to spurious patterns in the training set rather than learning robust general behaviors.\n",
    "- In addition, the base model used for fine-tuning is `gpt-4o-mini`, which is a fast yet weak model. Even with fine-tuning, a smaller-capacity model may struggle to internalize subtle syntactic and semantic constraints of shell commands from such a limited corpus, and can easily learn brittle heuristics that do not generalize to the held-out evaluation set.\n"
   ],
   "id": "5d2d59e5ee45b9c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "end_timestamp = datetime.now()\n",
    "duration = end_timestamp - start_timestamp\n",
    "\n",
    "print(f\"End time: {end_timestamp.strftime('%H:%M:%S')}\")\n",
    "print(f\"Time passed: {duration}\")"
   ],
   "id": "77854cf67a00e114",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
